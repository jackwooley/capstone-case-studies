2023-04-04 17:08:54,381 ----------------------------------------------------------------------------------------------------
2023-04-04 17:08:54,381 Model: "TextClassifier(
  (embeddings): TransformerDocumentEmbeddings(
    (model): DistilBertModel(
      (embeddings): Embeddings(
        (word_embeddings): Embedding(30523, 768)
        (position_embeddings): Embedding(512, 768)
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (transformer): Transformer(
        (layer): ModuleList(
          (0-5): 6 x TransformerBlock(
            (attention): MultiHeadSelfAttention(
              (dropout): Dropout(p=0.1, inplace=False)
              (q_lin): Linear(in_features=768, out_features=768, bias=True)
              (k_lin): Linear(in_features=768, out_features=768, bias=True)
              (v_lin): Linear(in_features=768, out_features=768, bias=True)
              (out_lin): Linear(in_features=768, out_features=768, bias=True)
            )
            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (ffn): FFN(
              (dropout): Dropout(p=0.1, inplace=False)
              (lin1): Linear(in_features=768, out_features=3072, bias=True)
              (lin2): Linear(in_features=3072, out_features=768, bias=True)
              (activation): GELUActivation()
            )
            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
      )
    )
  )
  (decoder): Linear(in_features=768, out_features=4, bias=True)
  (dropout): Dropout(p=0.0, inplace=False)
  (locked_dropout): LockedDropout(p=0.0)
  (word_dropout): WordDropout(p=0.0)
  (loss_function): CrossEntropyLoss()
  (weights): None
  (weight_tensor) None
)"
2023-04-04 17:08:54,381 ----------------------------------------------------------------------------------------------------
2023-04-04 17:08:54,381 Corpus: "Corpus: 3832 train + 959 dev + 1198 test sentences"
2023-04-04 17:08:54,383 ----------------------------------------------------------------------------------------------------
2023-04-04 17:08:54,383 Parameters:
2023-04-04 17:08:54,383  - learning_rate: "0.000050"
2023-04-04 17:08:54,383  - mini_batch_size: "4"
2023-04-04 17:08:54,383  - patience: "3"
2023-04-04 17:08:54,383  - anneal_factor: "0.5"
2023-04-04 17:08:54,383  - max_epochs: "10"
2023-04-04 17:08:54,383  - shuffle: "True"
2023-04-04 17:08:54,383  - train_with_dev: "False"
2023-04-04 17:08:54,383  - batch_growth_annealing: "False"
2023-04-04 17:08:54,383 ----------------------------------------------------------------------------------------------------
2023-04-04 17:08:54,383 Model training base path: "test_distilbert_3_bucket"
2023-04-04 17:08:54,383 ----------------------------------------------------------------------------------------------------
2023-04-04 17:08:54,383 Device: cuda:0
2023-04-04 17:08:54,383 ----------------------------------------------------------------------------------------------------
2023-04-04 17:08:54,383 Embeddings storage mode: none
2023-04-04 17:08:54,383 ----------------------------------------------------------------------------------------------------
2023-04-04 17:09:01,328 epoch 1 - iter 95/958 - loss 1.09081629 - time (sec): 6.95 - samples/sec: 54.71 - lr: 0.000005
2023-04-04 17:09:06,441 epoch 1 - iter 190/958 - loss 0.94529482 - time (sec): 12.06 - samples/sec: 63.03 - lr: 0.000010
2023-04-04 17:09:12,032 epoch 1 - iter 285/958 - loss 0.79553228 - time (sec): 17.65 - samples/sec: 64.59 - lr: 0.000015
2023-04-04 17:09:17,457 epoch 1 - iter 380/958 - loss 0.74096139 - time (sec): 23.07 - samples/sec: 65.87 - lr: 0.000020
2023-04-04 17:09:22,823 epoch 1 - iter 475/958 - loss 0.71792315 - time (sec): 28.44 - samples/sec: 66.81 - lr: 0.000025
2023-04-04 17:09:28,108 epoch 1 - iter 570/958 - loss 0.70445619 - time (sec): 33.73 - samples/sec: 67.60 - lr: 0.000030
2023-04-04 17:09:34,478 epoch 1 - iter 665/958 - loss 0.68654236 - time (sec): 40.10 - samples/sec: 66.34 - lr: 0.000035
2023-04-04 17:09:39,740 epoch 1 - iter 760/958 - loss 0.67690140 - time (sec): 45.36 - samples/sec: 67.02 - lr: 0.000040
2023-04-04 17:09:45,406 epoch 1 - iter 855/958 - loss 0.66129758 - time (sec): 51.02 - samples/sec: 67.03 - lr: 0.000045
2023-04-04 17:09:51,332 epoch 1 - iter 950/958 - loss 0.65110452 - time (sec): 56.95 - samples/sec: 66.73 - lr: 0.000050
2023-04-04 17:09:51,785 ----------------------------------------------------------------------------------------------------
2023-04-04 17:09:51,785 EPOCH 1 done: loss 0.6497 - lr 0.000050
2023-04-04 17:09:56,765 Evaluating as a multi-label problem: False
2023-04-04 17:09:56,771 DEV : loss 0.5517280101776123 - f1-score (micro avg)  0.8457
2023-04-04 17:09:56,781 ----------------------------------------------------------------------------------------------------
2023-04-04 17:10:02,239 epoch 2 - iter 95/958 - loss 0.45071998 - time (sec): 5.46 - samples/sec: 69.62 - lr: 0.000049
2023-04-04 17:10:07,785 epoch 2 - iter 190/958 - loss 0.51698180 - time (sec): 11.00 - samples/sec: 69.07 - lr: 0.000049
2023-04-04 17:10:14,376 epoch 2 - iter 285/958 - loss 0.48481569 - time (sec): 17.60 - samples/sec: 64.79 - lr: 0.000048
2023-04-04 17:10:20,562 epoch 2 - iter 380/958 - loss 0.46246097 - time (sec): 23.78 - samples/sec: 63.92 - lr: 0.000048
2023-04-04 17:10:25,928 epoch 2 - iter 475/958 - loss 0.46791648 - time (sec): 29.15 - samples/sec: 65.19 - lr: 0.000047
2023-04-04 17:10:32,062 epoch 2 - iter 570/958 - loss 0.47751705 - time (sec): 35.28 - samples/sec: 64.62 - lr: 0.000047
2023-04-04 17:10:37,499 epoch 2 - iter 665/958 - loss 0.47332246 - time (sec): 40.72 - samples/sec: 65.33 - lr: 0.000046
2023-04-04 17:10:43,260 epoch 2 - iter 760/958 - loss 0.46205665 - time (sec): 46.48 - samples/sec: 65.41 - lr: 0.000046
2023-04-04 17:10:49,817 epoch 2 - iter 855/958 - loss 0.44774379 - time (sec): 53.04 - samples/sec: 64.48 - lr: 0.000045
2023-04-04 17:10:56,477 epoch 2 - iter 950/958 - loss 0.43663135 - time (sec): 59.70 - samples/sec: 63.66 - lr: 0.000044
2023-04-04 17:10:56,965 ----------------------------------------------------------------------------------------------------
2023-04-04 17:10:56,965 EPOCH 2 done: loss 0.4413 - lr 0.000044
2023-04-04 17:11:01,672 Evaluating as a multi-label problem: False
2023-04-04 17:11:01,681 DEV : loss 0.7452936172485352 - f1-score (micro avg)  0.8582
2023-04-04 17:11:01,686 ----------------------------------------------------------------------------------------------------
2023-04-04 17:11:08,198 epoch 3 - iter 95/958 - loss 0.21510348 - time (sec): 6.51 - samples/sec: 58.35 - lr: 0.000044
2023-04-04 17:11:14,728 epoch 3 - iter 190/958 - loss 0.19767996 - time (sec): 13.04 - samples/sec: 58.27 - lr: 0.000043
2023-04-04 17:11:20,562 epoch 3 - iter 285/958 - loss 0.21007222 - time (sec): 18.88 - samples/sec: 60.39 - lr: 0.000043
2023-04-04 17:11:26,041 epoch 3 - iter 380/958 - loss 0.21185991 - time (sec): 24.36 - samples/sec: 62.41 - lr: 0.000042
2023-04-04 17:11:33,089 epoch 3 - iter 475/958 - loss 0.19891522 - time (sec): 31.40 - samples/sec: 60.51 - lr: 0.000042
2023-04-04 17:11:38,688 epoch 3 - iter 570/958 - loss 0.19777883 - time (sec): 37.00 - samples/sec: 61.62 - lr: 0.000041
2023-04-04 17:11:44,158 epoch 3 - iter 665/958 - loss 0.19652441 - time (sec): 42.47 - samples/sec: 62.63 - lr: 0.000041
2023-04-04 17:11:49,656 epoch 3 - iter 760/958 - loss 0.20776585 - time (sec): 47.97 - samples/sec: 63.37 - lr: 0.000040
2023-04-04 17:11:55,928 epoch 3 - iter 855/958 - loss 0.20420901 - time (sec): 54.24 - samples/sec: 63.05 - lr: 0.000039
2023-04-04 17:12:02,594 epoch 3 - iter 950/958 - loss 0.20545514 - time (sec): 60.91 - samples/sec: 62.39 - lr: 0.000039
2023-04-04 17:12:03,052 ----------------------------------------------------------------------------------------------------
2023-04-04 17:12:03,052 EPOCH 3 done: loss 0.2056 - lr 0.000039
2023-04-04 17:12:08,636 Evaluating as a multi-label problem: False
2023-04-04 17:12:08,651 DEV : loss 0.6424964666366577 - f1-score (micro avg)  0.8853
2023-04-04 17:12:08,655 ----------------------------------------------------------------------------------------------------
2023-04-04 17:12:15,166 epoch 4 - iter 95/958 - loss 0.07528654 - time (sec): 6.51 - samples/sec: 58.37 - lr: 0.000038
2023-04-04 17:12:20,679 epoch 4 - iter 190/958 - loss 0.05374235 - time (sec): 12.02 - samples/sec: 63.21 - lr: 0.000038
2023-04-04 17:12:26,176 epoch 4 - iter 285/958 - loss 0.05258040 - time (sec): 17.52 - samples/sec: 65.07 - lr: 0.000037
2023-04-04 17:12:31,975 epoch 4 - iter 380/958 - loss 0.05065853 - time (sec): 23.32 - samples/sec: 65.18 - lr: 0.000037
2023-04-04 17:12:37,623 epoch 4 - iter 475/958 - loss 0.04682503 - time (sec): 28.97 - samples/sec: 65.59 - lr: 0.000036
2023-04-04 17:12:43,135 epoch 4 - iter 570/958 - loss 0.06282125 - time (sec): 34.48 - samples/sec: 66.13 - lr: 0.000036
2023-04-04 17:12:49,020 epoch 4 - iter 665/958 - loss 0.06453796 - time (sec): 40.36 - samples/sec: 65.90 - lr: 0.000035
2023-04-04 17:12:56,721 epoch 4 - iter 760/958 - loss 0.06898471 - time (sec): 48.07 - samples/sec: 63.25 - lr: 0.000034
2023-04-04 17:13:02,438 epoch 4 - iter 855/958 - loss 0.07164725 - time (sec): 53.78 - samples/sec: 63.59 - lr: 0.000034
2023-04-04 17:13:08,602 epoch 4 - iter 950/958 - loss 0.07313613 - time (sec): 59.95 - samples/sec: 63.39 - lr: 0.000033
2023-04-04 17:13:09,100 ----------------------------------------------------------------------------------------------------
2023-04-04 17:13:09,100 EPOCH 4 done: loss 0.0725 - lr 0.000033
2023-04-04 17:13:14,745 Evaluating as a multi-label problem: False
2023-04-04 17:13:14,754 DEV : loss 0.8730950355529785 - f1-score (micro avg)  0.8749
2023-04-04 17:13:14,759 ----------------------------------------------------------------------------------------------------
2023-04-04 17:13:20,423 epoch 5 - iter 95/958 - loss 0.03394586 - time (sec): 5.66 - samples/sec: 67.09 - lr: 0.000033
2023-04-04 17:13:26,035 epoch 5 - iter 190/958 - loss 0.01914277 - time (sec): 11.28 - samples/sec: 67.40 - lr: 0.000032
2023-04-04 17:13:33,598 epoch 5 - iter 285/958 - loss 0.02693978 - time (sec): 18.84 - samples/sec: 60.51 - lr: 0.000032
2023-04-04 17:13:39,366 epoch 5 - iter 380/958 - loss 0.02594219 - time (sec): 24.61 - samples/sec: 61.77 - lr: 0.000031
2023-04-04 17:13:44,794 epoch 5 - iter 475/958 - loss 0.02642708 - time (sec): 30.04 - samples/sec: 63.26 - lr: 0.000031
2023-04-04 17:13:52,555 epoch 5 - iter 570/958 - loss 0.02755389 - time (sec): 37.80 - samples/sec: 60.32 - lr: 0.000030
2023-04-04 17:13:58,164 epoch 5 - iter 665/958 - loss 0.02426441 - time (sec): 43.41 - samples/sec: 61.28 - lr: 0.000029
2023-04-04 17:14:04,342 epoch 5 - iter 760/958 - loss 0.02494597 - time (sec): 49.58 - samples/sec: 61.31 - lr: 0.000029
2023-04-04 17:14:11,662 epoch 5 - iter 855/958 - loss 0.02263890 - time (sec): 56.90 - samples/sec: 60.10 - lr: 0.000028
2023-04-04 17:14:17,753 epoch 5 - iter 950/958 - loss 0.02039323 - time (sec): 62.99 - samples/sec: 60.32 - lr: 0.000028
2023-04-04 17:14:18,198 ----------------------------------------------------------------------------------------------------
2023-04-04 17:14:18,198 EPOCH 5 done: loss 0.0202 - lr 0.000028
2023-04-04 17:14:22,524 Evaluating as a multi-label problem: False
2023-04-04 17:14:22,532 DEV : loss 1.127150535583496 - f1-score (micro avg)  0.8738
2023-04-04 17:14:22,540 ----------------------------------------------------------------------------------------------------
2023-04-04 17:14:28,052 epoch 6 - iter 95/958 - loss 0.00371716 - time (sec): 5.51 - samples/sec: 68.95 - lr: 0.000027
2023-04-04 17:14:34,351 epoch 6 - iter 190/958 - loss 0.00186869 - time (sec): 11.81 - samples/sec: 64.35 - lr: 0.000027
2023-04-04 17:14:40,438 epoch 6 - iter 285/958 - loss 0.00125511 - time (sec): 17.90 - samples/sec: 63.70 - lr: 0.000026
2023-04-04 17:14:46,147 epoch 6 - iter 380/958 - loss 0.00397703 - time (sec): 23.61 - samples/sec: 64.39 - lr: 0.000026
2023-04-04 17:14:53,567 epoch 6 - iter 475/958 - loss 0.00333352 - time (sec): 31.03 - samples/sec: 61.24 - lr: 0.000025
2023-04-04 17:14:59,378 epoch 6 - iter 570/958 - loss 0.00280742 - time (sec): 36.84 - samples/sec: 61.89 - lr: 0.000024
2023-04-04 17:15:06,158 epoch 6 - iter 665/958 - loss 0.00916517 - time (sec): 43.62 - samples/sec: 60.98 - lr: 0.000024
2023-04-04 17:15:12,107 epoch 6 - iter 760/958 - loss 0.01292696 - time (sec): 49.57 - samples/sec: 61.33 - lr: 0.000023
2023-04-04 17:15:17,873 epoch 6 - iter 855/958 - loss 0.01153922 - time (sec): 55.33 - samples/sec: 61.81 - lr: 0.000023
2023-04-04 17:15:24,155 epoch 6 - iter 950/958 - loss 0.01085481 - time (sec): 61.61 - samples/sec: 61.67 - lr: 0.000022
2023-04-04 17:15:24,590 ----------------------------------------------------------------------------------------------------
2023-04-04 17:15:24,590 EPOCH 6 done: loss 0.0108 - lr 0.000022
2023-04-04 17:15:29,819 Evaluating as a multi-label problem: False
2023-04-04 17:15:29,827 DEV : loss 1.3162732124328613 - f1-score (micro avg)  0.8728
2023-04-04 17:15:29,835 ----------------------------------------------------------------------------------------------------
2023-04-04 17:15:36,421 epoch 7 - iter 95/958 - loss 0.00052531 - time (sec): 6.59 - samples/sec: 57.70 - lr: 0.000022
2023-04-04 17:15:41,918 epoch 7 - iter 190/958 - loss 0.00032774 - time (sec): 12.08 - samples/sec: 62.90 - lr: 0.000021
2023-04-04 17:15:47,479 epoch 7 - iter 285/958 - loss 0.00022000 - time (sec): 17.64 - samples/sec: 64.61 - lr: 0.000021
2023-04-04 17:15:53,278 epoch 7 - iter 380/958 - loss 0.00016652 - time (sec): 23.44 - samples/sec: 64.84 - lr: 0.000020
2023-04-04 17:15:59,152 epoch 7 - iter 475/958 - loss 0.00258562 - time (sec): 29.32 - samples/sec: 64.81 - lr: 0.000019
2023-04-04 17:16:05,498 epoch 7 - iter 570/958 - loss 0.00680930 - time (sec): 35.66 - samples/sec: 63.93 - lr: 0.000019
2023-04-04 17:16:13,017 epoch 7 - iter 665/958 - loss 0.00588882 - time (sec): 43.18 - samples/sec: 61.60 - lr: 0.000018
2023-04-04 17:16:19,464 epoch 7 - iter 760/958 - loss 0.00517820 - time (sec): 49.63 - samples/sec: 61.25 - lr: 0.000018
2023-04-04 17:16:25,522 epoch 7 - iter 855/958 - loss 0.00460699 - time (sec): 55.69 - samples/sec: 61.41 - lr: 0.000017
2023-04-04 17:16:31,431 epoch 7 - iter 950/958 - loss 0.00533962 - time (sec): 61.60 - samples/sec: 61.69 - lr: 0.000017
2023-04-04 17:16:31,867 ----------------------------------------------------------------------------------------------------
2023-04-04 17:16:31,867 EPOCH 7 done: loss 0.0053 - lr 0.000017
2023-04-04 17:16:36,559 Evaluating as a multi-label problem: False
2023-04-04 17:16:36,568 DEV : loss 1.7893882989883423 - f1-score (micro avg)  0.8498
2023-04-04 17:16:36,573 ----------------------------------------------------------------------------------------------------
2023-04-04 17:16:42,115 epoch 8 - iter 95/958 - loss 0.05558247 - time (sec): 5.54 - samples/sec: 68.59 - lr: 0.000016
2023-04-04 17:16:47,605 epoch 8 - iter 190/958 - loss 0.03489601 - time (sec): 11.03 - samples/sec: 68.90 - lr: 0.000016
2023-04-04 17:16:54,934 epoch 8 - iter 285/958 - loss 0.02415948 - time (sec): 18.36 - samples/sec: 62.09 - lr: 0.000015
2023-04-04 17:17:00,725 epoch 8 - iter 380/958 - loss 0.01888225 - time (sec): 24.15 - samples/sec: 62.94 - lr: 0.000014
2023-04-04 17:17:06,262 epoch 8 - iter 475/958 - loss 0.01749384 - time (sec): 29.69 - samples/sec: 64.00 - lr: 0.000014
2023-04-04 17:17:12,299 epoch 8 - iter 570/958 - loss 0.01458124 - time (sec): 35.72 - samples/sec: 63.82 - lr: 0.000013
2023-04-04 17:17:19,437 epoch 8 - iter 665/958 - loss 0.01463206 - time (sec): 42.86 - samples/sec: 62.06 - lr: 0.000013
2023-04-04 17:17:25,460 epoch 8 - iter 760/958 - loss 0.01280352 - time (sec): 48.89 - samples/sec: 62.19 - lr: 0.000012
2023-04-04 17:17:33,333 epoch 8 - iter 855/958 - loss 0.01140124 - time (sec): 56.76 - samples/sec: 60.26 - lr: 0.000012
2023-04-04 17:17:41,379 epoch 8 - iter 950/958 - loss 0.01029695 - time (sec): 64.80 - samples/sec: 58.64 - lr: 0.000011
2023-04-04 17:17:41,829 ----------------------------------------------------------------------------------------------------
2023-04-04 17:17:41,830 EPOCH 8 done: loss 0.0102 - lr 0.000011
2023-04-04 17:17:47,414 Evaluating as a multi-label problem: False
2023-04-04 17:17:47,423 DEV : loss 1.4527853727340698 - f1-score (micro avg)  0.8655
2023-04-04 17:17:47,429 ----------------------------------------------------------------------------------------------------
2023-04-04 17:17:54,257 epoch 9 - iter 95/958 - loss 0.00171327 - time (sec): 6.83 - samples/sec: 55.66 - lr: 0.000011
2023-04-04 17:18:01,838 epoch 9 - iter 190/958 - loss 0.00085806 - time (sec): 14.41 - samples/sec: 52.75 - lr: 0.000010
2023-04-04 17:18:07,742 epoch 9 - iter 285/958 - loss 0.00057258 - time (sec): 20.31 - samples/sec: 56.12 - lr: 0.000009
2023-04-04 17:18:15,564 epoch 9 - iter 380/958 - loss 0.00129201 - time (sec): 28.13 - samples/sec: 54.03 - lr: 0.000009
2023-04-04 17:18:21,518 epoch 9 - iter 475/958 - loss 0.00104183 - time (sec): 34.09 - samples/sec: 55.74 - lr: 0.000008
2023-04-04 17:18:27,547 epoch 9 - iter 570/958 - loss 0.00088068 - time (sec): 40.12 - samples/sec: 56.83 - lr: 0.000008
2023-04-04 17:18:33,345 epoch 9 - iter 665/958 - loss 0.00075672 - time (sec): 45.92 - samples/sec: 57.93 - lr: 0.000007
2023-04-04 17:18:39,812 epoch 9 - iter 760/958 - loss 0.00427353 - time (sec): 52.38 - samples/sec: 58.03 - lr: 0.000007
2023-04-04 17:18:45,671 epoch 9 - iter 855/958 - loss 0.00458212 - time (sec): 58.24 - samples/sec: 58.72 - lr: 0.000006
2023-04-04 17:18:54,171 epoch 9 - iter 950/958 - loss 0.00412441 - time (sec): 66.74 - samples/sec: 56.94 - lr: 0.000006
2023-04-04 17:18:54,582 ----------------------------------------------------------------------------------------------------
2023-04-04 17:18:54,582 EPOCH 9 done: loss 0.0041 - lr 0.000006
2023-04-04 17:18:59,389 Evaluating as a multi-label problem: False
2023-04-04 17:18:59,398 DEV : loss 1.4635103940963745 - f1-score (micro avg)  0.8728
2023-04-04 17:18:59,407 ----------------------------------------------------------------------------------------------------
2023-04-04 17:19:04,859 epoch 10 - iter 95/958 - loss 0.00039655 - time (sec): 5.45 - samples/sec: 69.75 - lr: 0.000005
2023-04-04 17:19:10,496 epoch 10 - iter 190/958 - loss 0.00019931 - time (sec): 11.09 - samples/sec: 68.53 - lr: 0.000004
2023-04-04 17:19:18,392 epoch 10 - iter 285/958 - loss 0.00013459 - time (sec): 18.99 - samples/sec: 60.05 - lr: 0.000004
2023-04-04 17:19:25,000 epoch 10 - iter 380/958 - loss 0.00010179 - time (sec): 25.59 - samples/sec: 59.39 - lr: 0.000003
2023-04-04 17:19:33,255 epoch 10 - iter 475/958 - loss 0.00008212 - time (sec): 33.85 - samples/sec: 56.13 - lr: 0.000003
2023-04-04 17:19:40,009 epoch 10 - iter 570/958 - loss 0.00006875 - time (sec): 40.60 - samples/sec: 56.15 - lr: 0.000002
2023-04-04 17:19:46,705 epoch 10 - iter 665/958 - loss 0.00008477 - time (sec): 47.30 - samples/sec: 56.24 - lr: 0.000002
2023-04-04 17:19:52,902 epoch 10 - iter 760/958 - loss 0.00007463 - time (sec): 53.50 - samples/sec: 56.83 - lr: 0.000001
2023-04-04 17:19:59,244 epoch 10 - iter 855/958 - loss 0.00006645 - time (sec): 59.84 - samples/sec: 57.15 - lr: 0.000001
2023-04-04 17:20:04,906 epoch 10 - iter 950/958 - loss 0.00007830 - time (sec): 65.50 - samples/sec: 58.02 - lr: 0.000000
2023-04-04 17:20:05,413 ----------------------------------------------------------------------------------------------------
2023-04-04 17:20:05,413 EPOCH 10 done: loss 0.0001 - lr 0.000000
2023-04-04 17:20:10,415 Evaluating as a multi-label problem: False
2023-04-04 17:20:10,424 DEV : loss 1.4975475072860718 - f1-score (micro avg)  0.8697
2023-04-04 17:20:10,905 ----------------------------------------------------------------------------------------------------
2023-04-04 17:20:10,905 Testing using last state of model ...
2023-04-04 17:20:17,069 Evaluating as a multi-label problem: False
2023-04-04 17:20:17,079 0.8623	0.8623	0.8623	0.8623
2023-04-04 17:20:17,079 
Results:
- F-score (micro) 0.8623
- F-score (macro) 0.8599
- Accuracy 0.8623

By class:
              precision    recall  f1-score   support

           1     0.9243    0.8661    0.8943       620
           3     0.7657    0.8453    0.8035       375
           5     0.8818    0.8818    0.8818       203

    accuracy                         0.8623      1198
   macro avg     0.8572    0.8644    0.8599      1198
weighted avg     0.8674    0.8623    0.8637      1198

2023-04-04 17:20:17,079 ----------------------------------------------------------------------------------------------------
